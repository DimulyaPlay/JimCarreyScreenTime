{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Jim Carrey Train.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "DEFaMczL-Z-t"
      },
      "source": [
        "import os \n",
        "import cv2\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.applications import VGG16, Xception, VGG19\n",
        "from tensorflow.keras.layers import Dense, Input, Lambda, Flatten, Reshape, Conv2D, Conv2DTranspose, UpSampling2D\n",
        "from tensorflow.keras.layers import Dropout, BatchNormalization, LeakyReLU, concatenate, MaxPooling2D\n",
        "from tensorflow.keras.models import Model, load_model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import numpy as np\n",
        "from tensorflow.keras.callbacks  import ModelCheckpoint\n",
        "from moviepy.editor import * \n",
        "from natsort import natsorted\n",
        "from tqdm import tqdm\n",
        "from IPython.display import clear_output"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lHocYk25ViFl",
        "cellView": "form",
        "outputId": "c1cb15b8-999e-4f43-a4fc-1af88e4d5b67"
      },
      "source": [
        "#@title Colab\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "zHT1xSuT0H4x"
      },
      "source": [
        "#@title Local\n",
        "os.chdir(\"E:/\")"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mYT2wFoC-wKd"
      },
      "source": [
        "## раскадровка фильмов для сбора базы. Ссылка на собранную базу https://drive.google.com/file/d/1MQ36P2OpQDoO62_nqkk7uuuKlPIcdCd5/view?usp=sharing\n",
        "\n",
        "count = 0                         \n",
        "imId = 'Мистер Судьба - 720HD - [ KinoMobi.net ].mp4'\n",
        "cap= cv2.VideoCapture(imId)\n",
        "frameRate = cap.get(5)*3\n",
        "image_folder = 'forsort'\n",
        "while cap.isOpened():\n",
        "  frameId = cap.get(1)\n",
        "  ret, frame = cap.read()\n",
        "  if (ret != True):\n",
        "    break\n",
        "  if (frameId % np.floor(frameRate) == 0):\n",
        "      filename =\"frameF%d.jpg\" % count;count+=1\n",
        "      cv2.imwrite(image_folder+'/'+ filename, frame)\n",
        "cap.release()\n",
        "cv2.destroyAllWindows()"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SJrN93QMwrUa"
      },
      "source": [
        "!unzip -q '/content/drive/MyDrive/Базы/Jim.zip'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D_La05M7yFBL"
      },
      "source": [
        "## vgg16"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w-N9JYWjp75p",
        "outputId": "66bc8bb8-e272-485d-ae58-175be9fe8592"
      },
      "source": [
        "train_datagen = ImageDataGenerator(rescale=1./255, validation_split= 0.2, horizontal_flip=True, rotation_range=5)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(directory='/content/Jim', target_size=(224,224), subset='training', batch_size=64)\n",
        "val_generator = train_datagen.flow_from_directory(directory='/content/Jim', target_size=(224,224), subset='validation', batch_size=64)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 9684 images belonging to 2 classes.\n",
            "Found 2420 images belonging to 2 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VjhCSA_uu7fG"
      },
      "source": [
        "vgg_model = VGG16(include_top=False)\n",
        "vggModel = Model(vgg_model.input,vgg_model.output)\n",
        "\n",
        "xc_model = Xception(include_top=False)\n",
        "xcModel = Model(xc_model.input,xc_model.output)\n",
        "\n",
        "inputs = Input(shape=(224,224,3))\n",
        "popOut = vggModel(inputs)\n",
        "flatten = Flatten()(popOut)\n",
        "dense1 = Dense(1024, activation = 'relu')(flatten)\n",
        "drop1 = Dropout(0.5)(dense1)\n",
        "dense2 = Dense(1024, activation = 'relu')(drop1)\n",
        "drop2 = Dropout(0.5)(dense2)\n",
        "outputs = Dense(2, activation = 'softmax')(drop2)\n",
        "model = Model(inputs, outputs)\n",
        "vggModel.trainable = False\n",
        "\n",
        "model.compile(optimizer=\"Adam\", loss=\"categorical_crossentropy\", metrics=['accuracy'])\n",
        "filepath=\"best_model.h5\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
        "callbacks_list = [checkpoint]"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rPYUmdCb0Fh5"
      },
      "source": [
        "model.fit(train_generator,  epochs=7, validation_data=val_generator, callbacks = callbacks_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vqwq5xGWK5KX",
        "outputId": "a41b988c-e3ef-4b83-91ee-782ea781406c"
      },
      "source": [
        "model.load_weights('best_model.h5')\n",
        "model.save('E:\\content\\drive\\MyDrive\\TrainedModels\\jimVGGVal78')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: E:\\content\\drive\\MyDrive\\TrainedModels\\jimVal78\\assets\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZjWpEf58yH1X"
      },
      "source": [
        "## Xception"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G-07eod7yCqq",
        "outputId": "350d97ea-86be-4d4e-c7ac-58bb1dba25c6"
      },
      "source": [
        "train_datagen = ImageDataGenerator(rescale=1./255, validation_split= 0.2, horizontal_flip=True, rotation_range=5)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(directory='/content/Jim', target_size=(229,229), subset='training', batch_size=64)\n",
        "val_generator = train_datagen.flow_from_directory(directory='/content/Jim', target_size=(229,229), subset='validation', batch_size=64)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 9684 images belonging to 2 classes.\n",
            "Found 2420 images belonging to 2 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "noOzmMcWyA36"
      },
      "source": [
        "xc_model = Xception(include_top=False)\n",
        "xcModel = Model(xc_model.input,xc_model.output)\n",
        "\n",
        "inputs = Input(shape=(229,229,3))\n",
        "popOut = xcModel(inputs)\n",
        "flatten = Flatten()(popOut)\n",
        "dense1 = Dense(1024, activation = 'relu')(flatten)\n",
        "drop1 = Dropout(0.5)(dense1)\n",
        "\n",
        "outputs = Dense(2, activation = 'softmax')(drop1)\n",
        "model = Model(inputs, outputs)\n",
        "xcModel.trainable = False\n",
        "\n",
        "model.compile(optimizer=\"Adam\", loss=\"categorical_crossentropy\", metrics=['accuracy'])\n",
        "filepath=\"best_model.h5\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
        "callbacks_list = [checkpoint]"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ca2uD5C2yghz",
        "outputId": "184b47c8-8d6d-4f13-fa06-258fd63da4e1"
      },
      "source": [
        "model.fit(train_generator,  epochs=10, validation_data=val_generator, callbacks = callbacks_list)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "152/152 [==============================] - 116s 749ms/step - loss: 2.0713 - accuracy: 0.7783 - val_loss: 0.6416 - val_accuracy: 0.7471\n",
            "\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.74711, saving model to best_model.h5\n",
            "Epoch 2/10\n",
            "152/152 [==============================] - 114s 747ms/step - loss: 0.3005 - accuracy: 0.8753 - val_loss: 0.5800 - val_accuracy: 0.7653\n",
            "\n",
            "Epoch 00002: val_accuracy improved from 0.74711 to 0.76529, saving model to best_model.h5\n",
            "Epoch 3/10\n",
            "152/152 [==============================] - 113s 747ms/step - loss: 0.2602 - accuracy: 0.8921 - val_loss: 0.6535 - val_accuracy: 0.7277\n",
            "\n",
            "Epoch 00003: val_accuracy did not improve from 0.76529\n",
            "Epoch 4/10\n",
            "152/152 [==============================] - 113s 746ms/step - loss: 0.2396 - accuracy: 0.9013 - val_loss: 0.5954 - val_accuracy: 0.7678\n",
            "\n",
            "Epoch 00004: val_accuracy improved from 0.76529 to 0.76777, saving model to best_model.h5\n",
            "Epoch 5/10\n",
            "152/152 [==============================] - 114s 747ms/step - loss: 0.2093 - accuracy: 0.9186 - val_loss: 0.6887 - val_accuracy: 0.7657\n",
            "\n",
            "Epoch 00005: val_accuracy did not improve from 0.76777\n",
            "Epoch 6/10\n",
            "152/152 [==============================] - 114s 750ms/step - loss: 0.1888 - accuracy: 0.9277 - val_loss: 0.6192 - val_accuracy: 0.7446\n",
            "\n",
            "Epoch 00006: val_accuracy did not improve from 0.76777\n",
            "Epoch 7/10\n",
            "152/152 [==============================] - 114s 747ms/step - loss: 0.1708 - accuracy: 0.9362 - val_loss: 0.7631 - val_accuracy: 0.7426\n",
            "\n",
            "Epoch 00007: val_accuracy did not improve from 0.76777\n",
            "Epoch 8/10\n",
            "152/152 [==============================] - 114s 748ms/step - loss: 0.1650 - accuracy: 0.9352 - val_loss: 0.7075 - val_accuracy: 0.7405\n",
            "\n",
            "Epoch 00008: val_accuracy did not improve from 0.76777\n",
            "Epoch 9/10\n",
            "152/152 [==============================] - 114s 752ms/step - loss: 0.1450 - accuracy: 0.9428 - val_loss: 0.7170 - val_accuracy: 0.7438\n",
            "\n",
            "Epoch 00009: val_accuracy did not improve from 0.76777\n",
            "Epoch 10/10\n",
            "152/152 [==============================] - 114s 748ms/step - loss: 0.1354 - accuracy: 0.9456 - val_loss: 0.6861 - val_accuracy: 0.7616\n",
            "\n",
            "Epoch 00010: val_accuracy did not improve from 0.76777\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x240fd768fd0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CJ4y7lOtytSz",
        "outputId": "d9b6403b-0529-4792-bbff-953ed4c143e6"
      },
      "source": [
        "model.load_weights('best_model.h5')\n",
        "model.save('E:\\content\\drive\\MyDrive\\TrainedModels\\jimXcVal77')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: E:\\content\\drive\\MyDrive\\TrainedModels\\jimXcVal77\\assets\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UFQlOJlGcDUa"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AirUgmg5cEHb"
      },
      "source": [
        "## vgg19\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qWCbvP2ucEHb",
        "outputId": "9d22a182-a9fb-427b-9e8e-2b2b339723c9"
      },
      "source": [
        "train_datagen = ImageDataGenerator(rescale=1./255, validation_split= 0.2, horizontal_flip=True, rotation_range=5)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(directory='/content/Jim', target_size=(224,224), subset='training', batch_size=64)\n",
        "val_generator = train_datagen.flow_from_directory(directory='/content/Jim', target_size=(224,224), subset='validation', batch_size=64)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 9684 images belonging to 2 classes.\n",
            "Found 2420 images belonging to 2 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C2eST4-RcEHc"
      },
      "source": [
        "vgg_model = VGG19(include_top=False)\n",
        "vggModel = Model(vgg_model.input,vgg_model.output)\n",
        "\n",
        "inputs = Input(shape=(224,224,3))\n",
        "popOut = vggModel(inputs)\n",
        "flatten = Flatten()(popOut)\n",
        "dense1 = Dense(1024, activation = 'relu')(flatten)\n",
        "drop1 = Dropout(0.5)(dense1)\n",
        "\n",
        "outputs = Dense(2, activation = 'softmax')(drop1)\n",
        "model = Model(inputs, outputs)\n",
        "vggModel.trainable = False\n",
        "\n",
        "model.compile(optimizer=\"Adam\", loss=\"categorical_crossentropy\", metrics=['accuracy'])\n",
        "filepath=\"best_model.h5\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
        "callbacks_list = [checkpoint]"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BP-81fNKcEHc",
        "outputId": "65900af9-0749-47f1-c9d8-edecf73485b0"
      },
      "source": [
        "model.fit(train_generator,  epochs=20, validation_data=val_generator, callbacks = callbacks_list)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "152/152 [==============================] - 127s 810ms/step - loss: 0.8997 - accuracy: 0.7120 - val_loss: 0.6216 - val_accuracy: 0.6789\n",
            "\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.67893, saving model to best_model.h5\n",
            "Epoch 2/20\n",
            "152/152 [==============================] - 121s 795ms/step - loss: 0.4104 - accuracy: 0.8105 - val_loss: 0.5651 - val_accuracy: 0.6934\n",
            "\n",
            "Epoch 00002: val_accuracy improved from 0.67893 to 0.69339, saving model to best_model.h5\n",
            "Epoch 3/20\n",
            "152/152 [==============================] - 122s 800ms/step - loss: 0.3830 - accuracy: 0.8256 - val_loss: 0.5882 - val_accuracy: 0.6847\n",
            "\n",
            "Epoch 00003: val_accuracy did not improve from 0.69339\n",
            "Epoch 4/20\n",
            "152/152 [==============================] - 121s 793ms/step - loss: 0.3621 - accuracy: 0.8427 - val_loss: 0.6350 - val_accuracy: 0.6579\n",
            "\n",
            "Epoch 00004: val_accuracy did not improve from 0.69339\n",
            "Epoch 5/20\n",
            "152/152 [==============================] - 120s 792ms/step - loss: 0.3783 - accuracy: 0.8267 - val_loss: 0.6082 - val_accuracy: 0.6905\n",
            "\n",
            "Epoch 00005: val_accuracy did not improve from 0.69339\n",
            "Epoch 6/20\n",
            "152/152 [==============================] - 123s 808ms/step - loss: 0.3354 - accuracy: 0.8519 - val_loss: 0.6327 - val_accuracy: 0.6740\n",
            "\n",
            "Epoch 00006: val_accuracy did not improve from 0.69339\n",
            "Epoch 7/20\n",
            "152/152 [==============================] - 122s 802ms/step - loss: 0.3836 - accuracy: 0.8159 - val_loss: 0.5969 - val_accuracy: 0.6723\n",
            "\n",
            "Epoch 00007: val_accuracy did not improve from 0.69339\n",
            "Epoch 8/20\n",
            "152/152 [==============================] - 123s 807ms/step - loss: 0.3791 - accuracy: 0.8111 - val_loss: 0.6237 - val_accuracy: 0.6831\n",
            "\n",
            "Epoch 00008: val_accuracy did not improve from 0.69339\n",
            "Epoch 9/20\n",
            "152/152 [==============================] - 120s 791ms/step - loss: 0.3657 - accuracy: 0.8215 - val_loss: 0.5920 - val_accuracy: 0.7050\n",
            "\n",
            "Epoch 00009: val_accuracy improved from 0.69339 to 0.70496, saving model to best_model.h5\n",
            "Epoch 10/20\n",
            "133/152 [=========================>....] - ETA: 13s - loss: 0.3498 - accuracy: 0.8160"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kINwa7XmcEHc",
        "outputId": "53ee0fa4-efff-436f-9e8b-ef991d785cf6"
      },
      "source": [
        "model.load_weights('best_model.h5')\n",
        "model.save('E:\\content\\drive\\MyDrive\\TrainedModels\\jimvgg19Val77')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: E:\\content\\drive\\MyDrive\\TrainedModels\\jimvgg19Val77\\assets\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A1sw7aYZ9Bo1"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P0gC2TJgao5v"
      },
      "source": [
        "## обработка"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LeddfqRlwAtR"
      },
      "source": [
        "def take_frames_from_video_by_seconds(model, video_path, divide_sec_on):\n",
        "  second = 0 \n",
        "  cap = cv2.VideoCapture(video_path)\n",
        "  frameRate = cap.get(5)/divide_sec_on\n",
        "  image_folder = 'temp'\n",
        "  while cap.isOpened():\n",
        "    frameId = cap.get(1)\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "      break\n",
        "    if frameId%round(frameRate,0)==1:  \n",
        "      filename =f\"{round(second, 1)}.jpg\";second+=(1/round(frameRate,0))\n",
        "      cv2.imwrite(image_folder+'/'+ filename, cv2.resize(frame, (224,224)))\n",
        "  cap.release()\n",
        "  cv2.destroyAllWindows()\n",
        "  return round(frameRate,0)\n",
        "  \n",
        "def predict_from_frames(frameRate, model=model, path='temp'):\n",
        "  counter = 0\n",
        "  predict = []\n",
        "  temp = []\n",
        "  for filename in tqdm(natsorted(os.listdir(path))):\n",
        "\n",
        "      img = cv2.imread(path+'/'+filename)\n",
        "      img = img/255.\n",
        "      pred = model.predict(img[None, :,:,:])\n",
        "      temp.append(np.argmax(pred[0]))\n",
        "      counter+=1\n",
        "      if counter>=frameRate:\n",
        "        predict.append(temp)\n",
        "        temp = []\n",
        "        counter = 0\n",
        "  return np.array(predict)\n",
        "\n",
        "def get_timings_from_predict(predict, frameRate):\n",
        "  seconds = []\n",
        "  for s, pred in enumerate(predict):\n",
        "    if sum(pred)<frameRate*0.4:\n",
        "      seconds.append(s)\n",
        "  return seconds\n",
        "\n",
        "def preprocess_timings(list_of_seconds, bias): # bias - разница между планами. Не разрезать кусок видео, если между от джима до джима промежуток bias секунд(0 - полная нетерпимость к отсутствию джима)\n",
        "  time_list = []\n",
        "  temp = []\n",
        "  for i in range(len(list_of_seconds)-1):\n",
        "    temp.append(list_of_seconds[i])\n",
        "    if list_of_seconds[i+1]-list_of_seconds[i]>bias+1:\n",
        "      time_list.append([temp[0], temp[-1]+1])\n",
        "      temp = []\n",
        "  return time_list\n",
        "\n",
        "def extract_samples(filepath, time_list):\n",
        "  for timings in time_list:\n",
        "    clip = VideoFileClip(filepath) \n",
        "    clip = clip.subclip(timings[0], timings[1]) \n",
        "    clip.write_videofile(filename = f\"cut/{timings[0]}.mp4\")\n",
        "    clear_output()\n",
        "    clip.close()\n",
        "\n",
        "def make_one_clip_from_cuts(cutdir=\"cut/\"):\n",
        "  clips =[]\n",
        "  for root, dirs, files in os.walk(cutdir):\n",
        "      files = natsorted(files)\n",
        "      for file in tqdm(files):\n",
        "        filePath = os.path.join(root, file)\n",
        "        video = VideoFileClip(filePath)\n",
        "        clips.append(video)\n",
        "  final_clip = concatenate_videoclips(clips)\n",
        "  final_clip.write_videofile('output.mp4', logger=None, verbose=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S6Ta4gnH2E7Z"
      },
      "source": [
        "def main(filepath,model=model, fps_division = 5, bias = 1):\n",
        "  try: os.mkdir('temp'); os.mkdir('cut')\n",
        "  except Exception:\n",
        "    pass\n",
        "  print('Подготовка к распознаванию')\n",
        "  divided_fps = take_frames_from_video_by_seconds(model, filepath, fps_division)\n",
        "  clear_output()\n",
        "  print('Распознавание')\n",
        "  predict = predict_from_frames(divided_fps)\n",
        "  clear_output()\n",
        "  print('Расчет длительности клипа')\n",
        "  seconds = get_timings_from_predict(predict, divided_fps)\n",
        "  time_list = preprocess_timings(seconds, bias)\n",
        "  clear_output()\n",
        "  print('Нарезка клипов')\n",
        "  extract_samples(filepath, time_list)\n",
        "  clear_output()\n",
        "  print('Склеивание клипов. Расчетное время Джима в кадре:',round(len(seconds)/60,0), \"минут\", len(seconds)%60, 'секунд')\n",
        "  make_one_clip_from_cuts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-vxxQR4k9ORO",
        "outputId": "5f01ea4d-3441-4ab5-efb9-7ffdc8ab9380"
      },
      "source": [
        "main('Shou_Trumana_Kinosimka.RU (online-video-cutter.com).mp4')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  1%|▌                                                                                 | 1/162 [00:00<00:32,  4.98it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Склеивание клипов. Расчетное время Джима в кадре: 7.0 минут 0 секунд\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|████████████████████████████████████████████████████████████████████████████████| 162/162 [00:32<00:00,  5.01it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}